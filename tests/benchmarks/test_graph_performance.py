"""Performance benchmark tests for graph execution.

This module uses pytest-benchmark to measure graph execution performance,
including invoke/stream latency, multi-node throughput, checkpointer overhead,
and message processing rates.
"""

import pytest

from agentflow.checkpointer import InMemoryCheckpointer
from agentflow.graph import StateGraph
from agentflow.state import AgentState, Message
from agentflow.utils import END


class TestSimpleGraphPerformance:
    """Benchmark simple graph execution patterns."""

    @pytest.mark.asyncio
    async def test_simple_graph_invoke_latency(self, benchmark):
        """Benchmark latency of simple single-node graph invoke."""
        def simple_node(state: AgentState) -> AgentState:
            state.context.append(Message.text_message("Response", role="assistant"))
            return state
        
        graph = StateGraph[AgentState](AgentState())
        graph.add_node("node", simple_node)
        graph.set_entry_point("node")
        graph.add_edge("node", END)
        
        compiled = graph.compile()
        
        input_data = {"messages": [Message.text_message("Test", role="user")]}
        config = {"thread_id": "benchmark_simple_invoke"}
        
        # Benchmark the invoke operation
        async def run_invoke():
            return await compiled.ainvoke(input_data, config)
        
        result = await benchmark.pedantic(
            run_invoke,
            iterations=100,
            rounds=5,
        )
        
        assert len(result["messages"]) >= 1

    @pytest.mark.asyncio
    async def test_simple_graph_stream_latency(self, benchmark):
        """Benchmark latency of simple single-node graph streaming."""
        def simple_node(state: AgentState) -> AgentState:
            state.context.append(Message.text_message("Response", role="assistant"))
            return state
        
        graph = StateGraph[AgentState](AgentState())
        graph.add_node("node", simple_node)
        graph.set_entry_point("node")
        graph.add_edge("node", END)
        
        compiled = graph.compile()
        
        input_data = {"messages": [Message.text_message("Test", role="user")]}
        config = {"thread_id": "benchmark_simple_stream"}
        
        # Benchmark the stream operation
        async def run_stream():
            chunks = []
            async for chunk in compiled.astream(input_data, config):
                chunks.append(chunk)
            return chunks
        
        result = await benchmark.pedantic(
            run_stream,
            iterations=100,
            rounds=5,
        )
        
        assert len(result) > 0


class TestMultiNodeGraphPerformance:
    """Benchmark multi-node graph execution."""

    @pytest.mark.asyncio
    async def test_linear_chain_performance(self, benchmark):
        """Benchmark performance of linear chain with 5 nodes."""
        def node_func(node_name: str):
            def func(state: AgentState) -> AgentState:
                state.context.append(
                    Message.text_message(f"Node {node_name}", role="assistant")
                )
                return state
            return func
        
        graph = StateGraph[AgentState](AgentState())
        
        # Create linear chain: node1 -> node2 -> node3 -> node4 -> node5 -> END
        nodes = ["node1", "node2", "node3", "node4", "node5"]
        for node_name in nodes:
            graph.add_node(node_name, node_func(node_name))
        
        graph.set_entry_point(nodes[0])
        for i in range(len(nodes) - 1):
            graph.add_edge(nodes[i], nodes[i + 1])
        graph.add_edge(nodes[-1], END)
        
        compiled = graph.compile()
        
        input_data = {"messages": [Message.text_message("Test", role="user")]}
        config = {"thread_id": "benchmark_linear_chain"}
        
        async def run_invoke():
            return await compiled.ainvoke(input_data, config)
        
        result = await benchmark.pedantic(
            run_invoke,
            iterations=50,
            rounds=3,
        )
        
        # Should have executed all 5 nodes
        assert len(result["messages"]) >= 5

    @pytest.mark.asyncio
    async def test_parallel_node_performance(self, benchmark):
        """Benchmark performance of graph with parallel execution paths."""
        def node_func(node_name: str):
            def func(state: AgentState) -> AgentState:
                state.context.append(
                    Message.text_message(f"Node {node_name}", role="assistant")
                )
                return state
            return func
        
        def router(state: AgentState) -> str:
            # Alternate between paths
            return "path_a" if len(state.context) % 2 == 0 else "path_b"
        
        graph = StateGraph[AgentState](AgentState())
        graph.add_node("router", node_func("router"))
        graph.add_node("path_a", node_func("path_a"))
        graph.add_node("path_b", node_func("path_b"))
        graph.add_node("merge", node_func("merge"))
        
        graph.set_entry_point("router")
        graph.add_conditional_edges(
            "router",
            router,
            {"path_a": "path_a", "path_b": "path_b"},
        )
        graph.add_edge("path_a", "merge")
        graph.add_edge("path_b", "merge")
        graph.add_edge("merge", END)
        
        compiled = graph.compile()
        
        input_data = {"messages": [Message.text_message("Test", role="user")]}
        config = {"thread_id": "benchmark_parallel"}
        
        async def run_invoke():
            return await compiled.ainvoke(input_data, config)
        
        result = await benchmark.pedantic(
            run_invoke,
            iterations=50,
            rounds=3,
        )
        
        assert len(result["messages"]) >= 1


class TestCheckpointerPerformance:
    """Benchmark checkpointer overhead."""

    @pytest.mark.asyncio
    async def test_no_checkpointer_baseline(self, benchmark):
        """Baseline performance without checkpointer."""
        def simple_node(state: AgentState) -> AgentState:
            state.context.append(Message.text_message("Response", role="assistant"))
            return state
        
        graph = StateGraph[AgentState](AgentState())
        graph.add_node("node", simple_node)
        graph.set_entry_point("node")
        graph.add_edge("node", END)
        
        # No checkpointer
        compiled = graph.compile()
        
        input_data = {"messages": [Message.text_message("Test", role="user")]}
        config = {"thread_id": "benchmark_no_checkpoint"}
        
        async def run_invoke():
            return await compiled.ainvoke(input_data, config)
        
        result = await benchmark.pedantic(
            run_invoke,
            iterations=100,
            rounds=5,
        )
        
        assert len(result["messages"]) >= 1

    @pytest.mark.asyncio
    async def test_in_memory_checkpointer_overhead(self, benchmark):
        """Measure InMemoryCheckpointer overhead."""
        checkpointer = InMemoryCheckpointer()
        
        def simple_node(state: AgentState) -> AgentState:
            state.context.append(Message.text_message("Response", role="assistant"))
            return state
        
        graph = StateGraph[AgentState](AgentState())
        graph.add_node("node", simple_node)
        graph.set_entry_point("node")
        graph.add_edge("node", END)
        
        # With InMemoryCheckpointer
        compiled = graph.compile(checkpointer=checkpointer)
        
        input_data = {"messages": [Message.text_message("Test", role="user")]}
        config = {"thread_id": "benchmark_checkpoint"}
        
        async def run_invoke():
            return await compiled.ainvoke(input_data, config)
        
        result = await benchmark.pedantic(
            run_invoke,
            iterations=100,
            rounds=5,
        )
        
        assert len(result["messages"]) >= 1

    @pytest.mark.asyncio
    async def test_checkpoint_save_load_performance(self, benchmark):
        """Benchmark checkpoint save/load cycle."""
        checkpointer = InMemoryCheckpointer()
        
        def node1(state: AgentState) -> AgentState:
            state.context.append(Message.text_message("Node1", role="assistant"))
            return state
        
        def node2(state: AgentState) -> AgentState:
            state.context.append(Message.text_message("Node2", role="assistant"))
            return state
        
        graph = StateGraph[AgentState](AgentState())
        graph.add_node("node1", node1)
        graph.add_node("node2", node2)
        
        graph.set_entry_point("node1")
        graph.add_edge("node1", "node2")
        graph.add_edge("node2", END)
        
        compiled = graph.compile(
            checkpointer=checkpointer,
            interrupt_after=["node1"],
        )
        
        thread_id = "benchmark_checkpoint_cycle"
        
        # Benchmark the save (first run) + load (second run) cycle
        async def run_checkpoint_cycle():
            # First run: saves checkpoint after node1
            await compiled.ainvoke(
                {"messages": [Message.text_message("Test", role="user")]},
                {"thread_id": thread_id},
            )
            # Second run: loads checkpoint and continues
            result = await compiled.ainvoke(
                {},
                {"thread_id": thread_id},
            )
            return result
        
        result = await benchmark.pedantic(
            run_checkpoint_cycle,
            iterations=50,
            rounds=3,
        )
        
        assert len(result["messages"]) >= 1


class TestMessageProcessingPerformance:
    """Benchmark message processing throughput."""

    @pytest.mark.asyncio
    async def test_small_message_batch_processing(self, benchmark):
        """Benchmark processing of small message batches (10 messages)."""
        def processing_node(state: AgentState) -> AgentState:
            # Process all messages in context
            for msg in state.context:
                # Simulate some processing
                _ = msg.text()
            
            state.context.append(
                Message.text_message("Processed batch", role="assistant")
            )
            return state
        
        graph = StateGraph[AgentState](AgentState())
        graph.add_node("processor", processing_node)
        graph.set_entry_point("processor")
        graph.add_edge("processor", END)
        
        compiled = graph.compile()
        
        # Create input with 10 messages
        messages = [
            Message.text_message(f"Message {i}", role="user")
            for i in range(10)
        ]
        input_data = {"messages": messages}
        config = {"thread_id": "benchmark_small_batch"}
        
        async def run_invoke():
            return await compiled.ainvoke(input_data, config)
        
        result = await benchmark.pedantic(
            run_invoke,
            iterations=50,
            rounds=3,
        )
        
        assert len(result["messages"]) >= 10

    @pytest.mark.asyncio
    async def test_large_message_batch_processing(self, benchmark):
        """Benchmark processing of large message batches (100 messages)."""
        def processing_node(state: AgentState) -> AgentState:
            # Process all messages in context
            for msg in state.context:
                _ = msg.text()
            
            state.context.append(
                Message.text_message("Processed large batch", role="assistant")
            )
            return state
        
        graph = StateGraph[AgentState](AgentState())
        graph.add_node("processor", processing_node)
        graph.set_entry_point("processor")
        graph.add_edge("processor", END)
        
        compiled = graph.compile()
        
        # Create input with 100 messages
        messages = [
            Message.text_message(f"Message {i}", role="user")
            for i in range(100)
        ]
        input_data = {"messages": messages}
        config = {"thread_id": "benchmark_large_batch"}
        
        async def run_invoke():
            return await compiled.ainvoke(input_data, config)
        
        result = await benchmark.pedantic(
            run_invoke,
            iterations=20,
            rounds=3,
        )
        
        assert len(result["messages"]) >= 100

    @pytest.mark.asyncio
    async def test_message_reducer_performance(self, benchmark):
        """Benchmark message reducer (add_messages) performance."""
        def add_message_node(state: AgentState) -> AgentState:
            # Add multiple messages that trigger reducer
            new_messages = [
                Message.text_message(f"New {i}", role="assistant")
                for i in range(10)
            ]
            state.context.extend(new_messages)
            return state
        
        graph = StateGraph[AgentState](AgentState())
        graph.add_node("adder", add_message_node)
        graph.set_entry_point("adder")
        graph.add_edge("adder", END)
        
        compiled = graph.compile()
        
        # Start with some existing messages
        initial_messages = [
            Message.text_message(f"Initial {i}", role="user")
            for i in range(10)
        ]
        input_data = {"messages": initial_messages}
        config = {"thread_id": "benchmark_reducer"}
        
        async def run_invoke():
            return await compiled.ainvoke(input_data, config)
        
        result = await benchmark.pedantic(
            run_invoke,
            iterations=50,
            rounds=3,
        )
        
        # Should have initial + new messages
        assert len(result["messages"]) >= 20


class TestRecursionPerformance:
    """Benchmark recursive/looping graph patterns."""

    @pytest.mark.asyncio
    async def test_controlled_loop_performance(self, benchmark):
        """Benchmark performance of controlled loop (10 iterations)."""
        def loop_node(state: AgentState) -> AgentState:
            # Track iteration via context_summary
            iteration = int(state.context_summary or "0")
            
            if iteration < 10:
                state.context_summary = str(iteration + 1)
                state.context.append(
                    Message.text_message(f"Iteration {iteration}", role="assistant")
                )
            
            return state
        
        def should_continue(state: AgentState) -> str:
            iteration = int(state.context_summary or "0")
            return "loop" if iteration < 10 else END
        
        graph = StateGraph[AgentState](AgentState())
        graph.add_node("loop", loop_node)
        
        graph.set_entry_point("loop")
        graph.add_conditional_edges(
            "loop",
            should_continue,
            {"loop": "loop", END: END},
        )
        
        compiled = graph.compile()
        
        input_data = {"messages": [Message.text_message("Start", role="user")]}
        config = {"thread_id": "benchmark_loop", "recursion_limit": 20}
        
        async def run_invoke():
            return await compiled.ainvoke(input_data, config)
        
        result = await benchmark.pedantic(
            run_invoke,
            iterations=30,
            rounds=3,
        )
        
        # Should have looped 10 times (initial message + 10 iteration messages)
        # Note: With LOW granularity, we only get messages, not state
        assert len(result["messages"]) >= 11  # 1 initial + 10 iterations


class TestStateSerializationPerformance:
    """Benchmark state serialization/deserialization."""

    @pytest.mark.asyncio
    async def test_small_state_serialization(self, benchmark):
        """Benchmark serialization of small state (5 messages)."""
        messages = [
            Message.text_message(f"Message {i}", role="user")
            for i in range(5)
        ]
        state = AgentState(context=messages)
        
        def serialize_deserialize():
            # Serialize to dict
            state_dict = state.model_dump()
            # Deserialize back
            restored = AgentState.model_validate(state_dict)
            return restored
        
        result = benchmark(serialize_deserialize)
        assert len(result.context) == 5

    @pytest.mark.asyncio
    async def test_large_state_serialization(self, benchmark):
        """Benchmark serialization of large state (100 messages)."""
        messages = [
            Message.text_message(f"Message {i}", role="user")
            for i in range(100)
        ]
        state = AgentState(context=messages)
        
        def serialize_deserialize():
            # Serialize to dict
            state_dict = state.model_dump()
            # Deserialize back
            restored = AgentState.model_validate(state_dict)
            return restored
        
        result = benchmark(serialize_deserialize)
        assert len(result.context) == 100
